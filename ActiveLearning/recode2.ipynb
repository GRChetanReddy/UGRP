{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook we try to implement the Active Learning algorithm to find self assembling tripeptides. We start out by importing important libraries for our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stuff\n",
    "import numpy as np                                      \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math,os,random,sys,h5py\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes a pandas dataframe and normalizes it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pandas_data(data):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    new_data = min_max_scaler.fit_transform(data)\n",
    "    return pd.DataFrame(new_data, index=data.index, columns=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we read the data from the files to use it. Since we have AP data for all peptides we use this data instead of running a simulation in each step.\n",
    "Note: 'sap' is a list of all self-assembling peptides as predicted by Tuttle et al.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "APs = pd.read_csv(\"tripeptide_AP.txt\", sep=\": \", index_col=0, header=None, engine = 'python')              #AP scores from simulation\n",
    "f = h5py.File('tripeptides.hdf5','r')                                                                      #reading data from the file generated from judred generator\n",
    "peps = np.array(f.get('peptides'))\n",
    "feas = np.array(f.get('features'))\n",
    "data = np.array(f.get('data'))\n",
    "f.close()\n",
    "judp = normalize_pandas_data(pd.DataFrame(data, columns = feas, index = peps))                              #arranging it into a pandas data frame\n",
    "smiles = pd.read_csv(\"smi.txt\", sep=\": \", engine='python')                                                  #smiles data of peptides by their names\n",
    "modp = normalize_pandas_data(pd.read_csv('Mordred_Parameters_to_be_used.csv', sep=',', engine='python'))    #mordred data in same order as the smiles data\n",
    "sap = ['PHE-PHE-PHE','TRP-PHE-PHE','PHE-TRP-PHE','PHE-PHE-TRP','ILE-PHE-TRP','PHE-TYR-ILE','PRO-TRP-PHE','TRP-PHE-LEU','ILE-PHE-PHE','VAL-PHE-TRP','PRO-PHE-PHE','PHE-PHE-MET','TRP-PHE-PHE','VAL-PHE-PHE','MET-PHE-PHE','TRP-LEU-LEU','SER-PHE-TRP','ILE-MET-TRP','LEU-CYS-PHE','SER-SER-PHE','SER-CYS-TRP','LYS-TRP-PHE','LYE-PHE-TRP','LYS-TRP-ASP','LYS-HSE-ASP','TRP-LYS-ASP','HSE-LYS-ASP','LYS-TYR-ASP','LYS-PHE-ASP','LYS-TRP-GLU','TRP-LYS-GLU','LYS-GLU-HSE','LYS-TYR-GLU','LYS-HSE-GLU','PRO-CYS-PHE','THR-SER-PHE','GLY-PHE-PHE','VAL-ALA-TRP']       #list of self assenbling peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALA-ALA-ALA</th>\n",
       "      <td>1.648900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASN-GLN-CYS</th>\n",
       "      <td>1.739705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALA-ALA-CYS</th>\n",
       "      <td>2.176944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRO-VAL-ALA</th>\n",
       "      <td>1.126057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL-PRO-ALA</th>\n",
       "      <td>1.095680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL-SER-ALA</th>\n",
       "      <td>1.381075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL-SER-GLN</th>\n",
       "      <td>1.664895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL-SER-GLU</th>\n",
       "      <td>1.168641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL-THR-GLN</th>\n",
       "      <td>1.180397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL-TRP-ARG</th>\n",
       "      <td>1.806671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    1\n",
       "0                    \n",
       "ALA-ALA-ALA  1.648900\n",
       "ASN-GLN-CYS  1.739705\n",
       "ALA-ALA-CYS  2.176944\n",
       "PRO-VAL-ALA  1.126057\n",
       "VAL-PRO-ALA  1.095680\n",
       "...               ...\n",
       "VAL-SER-ALA  1.381075\n",
       "VAL-SER-GLN  1.664895\n",
       "VAL-SER-GLU  1.168641\n",
       "VAL-THR-GLN  1.180397\n",
       "VAL-TRP-ARG  1.806671\n",
       "\n",
       "[8000 rows x 1 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b'SP2'</th>\n",
       "      <th>b'NH2'</th>\n",
       "      <th>b'MW'</th>\n",
       "      <th>b'S'</th>\n",
       "      <th>b'LogP WW'</th>\n",
       "      <th>b'Z'</th>\n",
       "      <th>b'MaxASA'</th>\n",
       "      <th>b'RotRatio'</th>\n",
       "      <th>b'Bulkiness'</th>\n",
       "      <th>b'OH'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b'ALA-ALA-CYS'</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.617271</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.314700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.583794</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.041781</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'ALA-ALA-ASP'</th>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.555590</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.138716</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.488029</td>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-0.106732</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'ALA-ALA-GLU'</th>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.483225</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.377532</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>-0.037767</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'ALA-ALA-PHE'</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.390007</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.480331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.314917</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.189564</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'ALA-ALA-GLY'</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.855167</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.124224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.815838</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.408867</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'TYR-TYR-THR'</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.322855</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.378882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421731</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.519066</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'TYR-TYR-VAL'</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.312687</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.511387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.429098</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.730706</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'TYR-TYR-TRP'</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.762155</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.451346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.837937</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.734355</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'TYR-TYR-TYR'</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.643233</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.601533</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'ALA-ALA-ALA'</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.782750</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.291926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.723757</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.113301</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  b'SP2'  b'NH2'     b'MW'      b'S'  b'LogP WW'      b'Z'  \\\n",
       "b'ALA-ALA-CYS' -1.000000    -1.0 -0.617271 -0.333333    0.314700  0.000000   \n",
       "b'ALA-ALA-ASP' -0.916667    -1.0 -0.555590 -1.000000   -0.138716 -0.333333   \n",
       "b'ALA-ALA-GLU' -0.916667    -1.0 -0.483225 -1.000000    0.026915 -0.333333   \n",
       "b'ALA-ALA-PHE' -0.500000    -1.0 -0.390007 -1.000000    0.480331  0.000000   \n",
       "b'ALA-ALA-GLY' -1.000000    -1.0 -0.855167 -1.000000    0.124224  0.000000   \n",
       "...                  ...     ...       ...       ...         ...       ...   \n",
       "b'TYR-TYR-THR'  0.000000    -1.0  0.322855 -1.000000    0.378882  0.000000   \n",
       "b'TYR-TYR-VAL'  0.000000    -1.0  0.312687 -1.000000    0.511387  0.000000   \n",
       "b'TYR-TYR-TRP'  0.666667    -1.0  0.762155 -1.000000    0.451346  0.000000   \n",
       "b'TYR-TYR-TYR'  0.500000    -1.0  0.643233 -1.000000    0.354037  0.000000   \n",
       "b'ALA-ALA-ALA' -1.000000    -1.0 -0.782750 -1.000000    0.291926  0.000000   \n",
       "\n",
       "                b'MaxASA'  b'RotRatio'  b'Bulkiness'     b'OH'  \n",
       "b'ALA-ALA-CYS'  -0.583794    -1.000000     -0.041781 -1.000000  \n",
       "b'ALA-ALA-ASP'  -0.488029    -0.916667     -0.106732 -1.000000  \n",
       "b'ALA-ALA-GLU'  -0.377532    -0.937500     -0.037767 -1.000000  \n",
       "b'ALA-ALA-PHE'  -0.314917    -0.500000      0.189564 -1.000000  \n",
       "b'ALA-ALA-GLY'  -0.815838    -1.000000     -0.408867 -1.000000  \n",
       "...                   ...          ...           ...       ...  \n",
       "b'TYR-TYR-THR'   0.421731    -0.250000      0.519066  1.000000  \n",
       "b'TYR-TYR-VAL'   0.429098    -0.400000      0.730706  0.333333  \n",
       "b'TYR-TYR-TRP'   0.837937     0.666667      0.734355  0.333333  \n",
       "b'TYR-TYR-TYR'   0.756906     0.500000      0.601533  1.000000  \n",
       "b'ALA-ALA-ALA'  -0.723757    -1.000000     -0.113301 -1.000000  \n",
       "\n",
       "[8000 rows x 10 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Peptides</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALA-ALA-ALA</td>\n",
       "      <td>[H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](C)C(O)=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALA-ALA-CYS</td>\n",
       "      <td>[H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CS)C(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALA-ALA-ASP</td>\n",
       "      <td>[H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CC(O)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALA-ALA-GLU</td>\n",
       "      <td>[H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALA-ALA-PHE</td>\n",
       "      <td>[H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CC1=C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>TYR-TYR-SER</td>\n",
       "      <td>[H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>TYR-TYR-THR</td>\n",
       "      <td>[H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>TYR-TYR-VAL</td>\n",
       "      <td>[H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>TYR-TYR-TRP</td>\n",
       "      <td>[H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>TYR-TYR-TYR</td>\n",
       "      <td>[H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Peptides                                             smiles\n",
       "0     ALA-ALA-ALA  [H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](C)C(O)=O\n",
       "1     ALA-ALA-CYS  [H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CS)C(...\n",
       "2     ALA-ALA-ASP  [H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CC(O)...\n",
       "3     ALA-ALA-GLU  [H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(O...\n",
       "4     ALA-ALA-PHE  [H]N[C@@H](C)C(=O)N[C@@H](C)C(=O)N[C@@H](CC1=C...\n",
       "...           ...                                                ...\n",
       "7995  TYR-TYR-SER  [H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...\n",
       "7996  TYR-TYR-THR  [H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...\n",
       "7997  TYR-TYR-VAL  [H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...\n",
       "7998  TYR-TYR-TRP  [H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...\n",
       "7999  TYR-TYR-TYR  [H]N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC...\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AATS0s</th>\n",
       "      <th>AATS2s</th>\n",
       "      <th>AATSC0are</th>\n",
       "      <th>AATSC0s</th>\n",
       "      <th>AATSC0se</th>\n",
       "      <th>AATSC1are</th>\n",
       "      <th>AATSC1pe</th>\n",
       "      <th>AATSC1v</th>\n",
       "      <th>AATSC2s</th>\n",
       "      <th>...</th>\n",
       "      <th>NsNH2</th>\n",
       "      <th>NsssN</th>\n",
       "      <th>RotRatio</th>\n",
       "      <th>SaasC</th>\n",
       "      <th>ETA_dEpsilon_B</th>\n",
       "      <th>ETA_shape_p</th>\n",
       "      <th>ETA_shape_y</th>\n",
       "      <th>Kier3</th>\n",
       "      <th>SpMAD_A</th>\n",
       "      <th>SlogP_VSA4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.232323</td>\n",
       "      <td>-0.252278</td>\n",
       "      <td>0.023694</td>\n",
       "      <td>-0.020400</td>\n",
       "      <td>-0.042373</td>\n",
       "      <td>0.242472</td>\n",
       "      <td>0.224388</td>\n",
       "      <td>-0.775307</td>\n",
       "      <td>0.081151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.174312</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.938632</td>\n",
       "      <td>0.615206</td>\n",
       "      <td>-0.594865</td>\n",
       "      <td>-0.999740</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.99975</td>\n",
       "      <td>-0.296009</td>\n",
       "      <td>-0.297832</td>\n",
       "      <td>-0.048211</td>\n",
       "      <td>-0.062218</td>\n",
       "      <td>-0.096375</td>\n",
       "      <td>0.298121</td>\n",
       "      <td>0.236236</td>\n",
       "      <td>-0.818339</td>\n",
       "      <td>0.054558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.009174</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.906403</td>\n",
       "      <td>0.232977</td>\n",
       "      <td>-0.580930</td>\n",
       "      <td>-0.753054</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.99950</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.224292</td>\n",
       "      <td>0.462118</td>\n",
       "      <td>0.419799</td>\n",
       "      <td>0.416672</td>\n",
       "      <td>-0.190430</td>\n",
       "      <td>-0.202021</td>\n",
       "      <td>-0.248698</td>\n",
       "      <td>0.481399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.708948</td>\n",
       "      <td>0.617650</td>\n",
       "      <td>-0.387127</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.99925</td>\n",
       "      <td>0.077482</td>\n",
       "      <td>0.044914</td>\n",
       "      <td>0.321717</td>\n",
       "      <td>0.288771</td>\n",
       "      <td>0.280594</td>\n",
       "      <td>0.010702</td>\n",
       "      <td>-0.012467</td>\n",
       "      <td>-0.337247</td>\n",
       "      <td>0.437974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173346</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.587540</td>\n",
       "      <td>0.410140</td>\n",
       "      <td>-0.283539</td>\n",
       "      <td>-0.814009</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.99900</td>\n",
       "      <td>-0.563853</td>\n",
       "      <td>-0.451082</td>\n",
       "      <td>-0.454447</td>\n",
       "      <td>-0.415300</td>\n",
       "      <td>-0.467848</td>\n",
       "      <td>0.590004</td>\n",
       "      <td>0.540307</td>\n",
       "      <td>-0.029802</td>\n",
       "      <td>-0.190107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.234362</td>\n",
       "      <td>-0.368639</td>\n",
       "      <td>0.015732</td>\n",
       "      <td>-0.015586</td>\n",
       "      <td>-0.042109</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.194774</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0.99900</td>\n",
       "      <td>-0.346334</td>\n",
       "      <td>-0.291621</td>\n",
       "      <td>-0.164071</td>\n",
       "      <td>-0.244333</td>\n",
       "      <td>-0.139386</td>\n",
       "      <td>0.167825</td>\n",
       "      <td>0.094548</td>\n",
       "      <td>0.900114</td>\n",
       "      <td>-0.468058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.256881</td>\n",
       "      <td>0.110087</td>\n",
       "      <td>0.631559</td>\n",
       "      <td>-0.411206</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>-0.207683</td>\n",
       "      <td>0.288059</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.99925</td>\n",
       "      <td>-0.422228</td>\n",
       "      <td>-0.347084</td>\n",
       "      <td>-0.239328</td>\n",
       "      <td>-0.309133</td>\n",
       "      <td>-0.215300</td>\n",
       "      <td>0.261171</td>\n",
       "      <td>0.187260</td>\n",
       "      <td>0.798861</td>\n",
       "      <td>-0.377276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.294412</td>\n",
       "      <td>0.106351</td>\n",
       "      <td>0.514444</td>\n",
       "      <td>-0.270064</td>\n",
       "      <td>0.208313</td>\n",
       "      <td>-0.176539</td>\n",
       "      <td>0.145550</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0.99950</td>\n",
       "      <td>-0.620089</td>\n",
       "      <td>-0.523298</td>\n",
       "      <td>-0.487674</td>\n",
       "      <td>-0.495633</td>\n",
       "      <td>-0.465288</td>\n",
       "      <td>0.519848</td>\n",
       "      <td>0.448797</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.279930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.294412</td>\n",
       "      <td>0.224857</td>\n",
       "      <td>0.368261</td>\n",
       "      <td>-0.225151</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>-0.176539</td>\n",
       "      <td>0.145550</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0.99975</td>\n",
       "      <td>-0.708493</td>\n",
       "      <td>-0.501028</td>\n",
       "      <td>-0.642080</td>\n",
       "      <td>-0.655456</td>\n",
       "      <td>-0.613518</td>\n",
       "      <td>0.537129</td>\n",
       "      <td>0.468147</td>\n",
       "      <td>0.898086</td>\n",
       "      <td>-0.423522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.457405</td>\n",
       "      <td>0.770275</td>\n",
       "      <td>0.938094</td>\n",
       "      <td>-0.727992</td>\n",
       "      <td>0.044843</td>\n",
       "      <td>-0.195957</td>\n",
       "      <td>0.644368</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.542864</td>\n",
       "      <td>-0.392993</td>\n",
       "      <td>-0.442680</td>\n",
       "      <td>-0.467078</td>\n",
       "      <td>-0.399720</td>\n",
       "      <td>0.406588</td>\n",
       "      <td>0.325715</td>\n",
       "      <td>0.932756</td>\n",
       "      <td>-0.360677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.377558</td>\n",
       "      <td>0.629706</td>\n",
       "      <td>0.893711</td>\n",
       "      <td>-0.585983</td>\n",
       "      <td>-0.013235</td>\n",
       "      <td>-0.026935</td>\n",
       "      <td>0.438452</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    AATS0s    AATS2s  AATSC0are   AATSC0s  AATSC0se  \\\n",
       "0       -1.00000 -0.232323 -0.252278   0.023694 -0.020400 -0.042373   \n",
       "1       -0.99975 -0.296009 -0.297832  -0.048211 -0.062218 -0.096375   \n",
       "2       -0.99950  0.246914  0.224292   0.462118  0.419799  0.416672   \n",
       "3       -0.99925  0.077482  0.044914   0.321717  0.288771  0.280594   \n",
       "4       -0.99900 -0.563853 -0.451082  -0.454447 -0.415300 -0.467848   \n",
       "...          ...       ...       ...        ...       ...       ...   \n",
       "7995     0.99900 -0.346334 -0.291621  -0.164071 -0.244333 -0.139386   \n",
       "7996     0.99925 -0.422228 -0.347084  -0.239328 -0.309133 -0.215300   \n",
       "7997     0.99950 -0.620089 -0.523298  -0.487674 -0.495633 -0.465288   \n",
       "7998     0.99975 -0.708493 -0.501028  -0.642080 -0.655456 -0.613518   \n",
       "7999     1.00000 -0.542864 -0.392993  -0.442680 -0.467078 -0.399720   \n",
       "\n",
       "      AATSC1are  AATSC1pe   AATSC1v   AATSC2s  ...  NsNH2  NsssN  RotRatio  \\\n",
       "0      0.242472  0.224388 -0.775307  0.081151  ...   -0.5   -1.0 -0.174312   \n",
       "1      0.298121  0.236236 -0.818339  0.054558  ...   -0.5   -1.0 -0.009174   \n",
       "2     -0.190430 -0.202021 -0.248698  0.481399  ...   -0.5   -1.0  0.045872   \n",
       "3      0.010702 -0.012467 -0.337247  0.437974  ...   -0.5   -1.0  0.173346   \n",
       "4      0.590004  0.540307 -0.029802 -0.190107  ...   -0.5   -1.0 -0.234362   \n",
       "...         ...       ...       ...       ...  ...    ...    ...       ...   \n",
       "7995   0.167825  0.094548  0.900114 -0.468058  ...   -0.5   -1.0 -0.256881   \n",
       "7996   0.261171  0.187260  0.798861 -0.377276  ...   -0.5   -1.0 -0.294412   \n",
       "7997   0.519848  0.448797  0.525072 -0.279930  ...   -0.5   -1.0 -0.294412   \n",
       "7998   0.537129  0.468147  0.898086 -0.423522  ...   -0.5   -1.0 -0.457405   \n",
       "7999   0.406588  0.325715  0.932756 -0.360677  ...   -0.5   -1.0 -0.377558   \n",
       "\n",
       "         SaasC  ETA_dEpsilon_B  ETA_shape_p  ETA_shape_y     Kier3   SpMAD_A  \\\n",
       "0    -1.000000       -1.000000     0.938632     0.615206 -0.594865 -0.999740   \n",
       "1    -1.000000       -1.000000     0.906403     0.232977 -0.580930 -0.753054   \n",
       "2    -1.000000       -1.000000     0.708948     0.617650 -0.387127 -1.000000   \n",
       "3    -1.000000       -1.000000     0.587540     0.410140 -0.283539 -0.814009   \n",
       "4    -0.368639        0.015732    -0.015586    -0.042109 -0.460807 -0.194774   \n",
       "...        ...             ...          ...          ...       ...       ...   \n",
       "7995  0.110087        0.631559    -0.411206     0.020975 -0.207683  0.288059   \n",
       "7996  0.106351        0.514444    -0.270064     0.208313 -0.176539  0.145550   \n",
       "7997  0.224857        0.368261    -0.225151     0.172017 -0.176539  0.145550   \n",
       "7998  0.770275        0.938094    -0.727992     0.044843 -0.195957  0.644368   \n",
       "7999  0.629706        0.893711    -0.585983    -0.013235 -0.026935  0.438452   \n",
       "\n",
       "      SlogP_VSA4  \n",
       "0      -1.000000  \n",
       "1      -1.000000  \n",
       "2      -1.000000  \n",
       "3      -1.000000  \n",
       "4      -1.000000  \n",
       "...          ...  \n",
       "7995   -1.000000  \n",
       "7996   -1.000000  \n",
       "7997   -0.333333  \n",
       "7998   -1.000000  \n",
       "7999   -1.000000  \n",
       "\n",
       "[8000 rows x 46 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-1.        , -1.        , -0.7827501 , -1.        ,  0.29192555,\n",
      "        0.        , -0.7237569 , -1.        , -0.11330056, -1.        ],\n",
      "      dtype=float32)]\n",
      "[array([-0.99974997, -0.29600933, -0.29783152, -0.04821083, -0.06221802,\n",
      "       -0.09637477,  0.29812115,  0.23623551, -0.81833914,  0.05455816,\n",
      "       -0.0476218 , -0.28325474, -0.38545507,  0.49770944,  0.42685493,\n",
      "        0.51218476, -0.18984381, -0.27525841, -0.23278923, -0.23719899,\n",
      "        0.24805536,  0.58168666, -0.17788701,  0.42878925, -0.85577319,\n",
      "       -0.12828389,  0.12129519,  0.1910701 , -0.4782197 , -0.80906926,\n",
      "        0.59273196,  0.30156102,  0.48439883, -0.18498607,  0.48837209,\n",
      "        0.48837209, -0.5       , -1.        , -0.00917431, -1.        ,\n",
      "       -1.        ,  0.90640259,  0.23297703, -0.58093009, -0.75305431,\n",
      "       -1.        ])]\n",
      "[1.64890034]\n"
     ]
    }
   ],
   "source": [
    "x_train_jd = [np.array(judp.loc[b'ALA-ALA-ALA'])]                  # initializing the training data with data of polyalanine \n",
    "x_train_md = [np.array(modp.iloc[1])]\n",
    "Y_train = np.array(APs.loc['ALA-ALA-ALA'])\n",
    "print(x_train_jd)\n",
    "print(x_train_md)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is a random peptide selector, it generates 10 random numbers between 1 to 8000 and uses these numbers as indices to select peptides from the data and adds it to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randPepSelector(APs, judp, modp, smiles, x_train_jd, x_train_md, Y_train):                           # a is APs, b is judp/modp\n",
    "    for pep in range(10):                                                                                # generates random indecies, looks up those indecies data and adds to the training data\n",
    "        i = random.randrange(1,8000)\n",
    "        p = APs.iloc[i]\n",
    "        print(p.name)\n",
    "        Y_train = np.append(Y_train, np.array(p), axis=0)\n",
    "        x_train_jd = np.append(x_train_jd, [np.array(judp.loc[p.name.encode('utf-8')])],axis=0)\n",
    "        x_train_md = np.append(x_train_md, np.array(modp.iloc[smiles.index[smiles['Peptides']==p.name]]), axis=0)\n",
    "    return Y_train,x_train_jd, x_train_md  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the random peptide selector and print the data to have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLN-ASP-SER\n",
      "GLY-HSE-ASP\n",
      "GLU-HSE-ALA\n",
      "ASP-TYR-ALA\n",
      "HSE-MET-ILE\n",
      "GLU-ARG-LEU\n",
      "ASP-SER-GLY\n",
      "VAL-THR-LEU\n",
      "LYS-ARG-SER\n",
      "LYS-GLU-ARG\n",
      "[[-1.         -1.         -0.7827501  -1.          0.29192555  0.\n",
      "  -0.7237569  -1.         -0.11330056 -1.        ]\n",
      " [-0.8333333  -0.6666666  -0.17858994 -1.         -0.10973078 -0.33333334\n",
      "  -0.03867412 -0.875      -0.07316196 -0.3333333 ]\n",
      " [-0.6666666  -1.         -0.28703415 -1.         -0.22567284 -0.33333334\n",
      "  -0.23020267 -0.5        -0.32238638 -1.        ]\n",
      " [-0.6666666  -1.         -0.14225233 -1.          0.1076605  -0.33333334\n",
      "  -0.02762437 -0.75        0.04214561 -1.        ]\n",
      " [-0.41666663 -1.         -0.08026218 -1.         -0.11801237 -0.33333334\n",
      "   0.00552487 -0.4166667   0.1315453  -0.3333333 ]\n",
      " [-0.75       -1.          0.08568192 -0.3333333   0.768116    0.\n",
      "   0.22651935 -0.90625     0.5011859  -1.        ]\n",
      " [-0.8333333  -0.3333333   0.17322206 -1.          0.09937891  0.\n",
      "   0.421731   -0.9444444   0.42492247 -1.        ]\n",
      " [-0.9166667  -1.         -0.54547334 -1.         -0.30641818 -0.33333334\n",
      "  -0.48434627 -0.875      -0.47637296 -0.3333333 ]\n",
      " [-1.         -1.         -0.26582003 -1.          0.7267082   0.\n",
      "  -0.13443828 -1.          0.7712096  -0.3333333 ]\n",
      " [-0.9166667   0.          0.033705   -1.         -0.15320903  0.6666667\n",
      "   0.300184   -0.96875     0.06768835 -0.3333333 ]\n",
      " [-0.8333333   0.          0.2506969  -1.         -0.41821945  0.33333334\n",
      "   0.55064464 -0.9444444   0.21729612 -1.        ]]\n",
      "[[-0.99974997 -0.29600933 -0.29783152 -0.04821083 -0.06221802 -0.09637477\n",
      "   0.29812115  0.23623551 -0.81833914  0.05455816 -0.0476218  -0.28325474\n",
      "  -0.38545507  0.49770944  0.42685493  0.51218476 -0.18984381 -0.27525841\n",
      "  -0.23278923 -0.23719899  0.24805536  0.58168666 -0.17788701  0.42878925\n",
      "  -0.85577319 -0.12828389  0.12129519  0.1910701  -0.4782197  -0.80906926\n",
      "   0.59273196  0.30156102  0.48439883 -0.18498607  0.48837209  0.48837209\n",
      "  -0.5        -1.         -0.00917431 -1.         -1.          0.90640259\n",
      "   0.23297703 -0.58093009 -0.75305431 -1.        ]\n",
      " [ 0.31391424  0.43341936  0.31092798  0.61352894  0.56957766  0.56490922\n",
      "  -0.60279963 -0.61318621  0.46532823  0.2089868   0.43711271 -0.36780813\n",
      "   0.57037037 -0.70211552 -0.70967918 -0.61020706 -0.14425908 -0.15583573\n",
      "   0.21212038  0.20687863 -0.06181007  0.70658806  0.28685251 -0.29208938\n",
      "   0.53733764 -0.59303744 -0.09561602 -0.80803633 -0.59636668 -0.38724425\n",
      "   0.39745381  0.21594237  0.32914289  0.08101307  0.23255814  0.23255814\n",
      "   0.         -1.          0.40007978 -1.         -1.          0.31678779\n",
      "   0.30280405 -0.05876351 -0.63012128 -1.        ]\n",
      " [-0.46943368  0.1300445   0.22945958  0.32229414  0.19958336  0.25927034\n",
      "  -0.33785681 -0.318583    0.10174701  0.12128731  0.44804292  0.09719328\n",
      "  -0.00945498 -0.30017515 -0.28479394 -0.21233863 -0.37579564  0.36009282\n",
      "   0.56646338  0.0975646  -0.27414361  0.20443588  0.17334519 -0.05709927\n",
      "   0.12624489 -0.23384184 -0.09258556 -0.84960805 -0.74922207 -0.02176316\n",
      "   0.15949727  0.72504101  0.49143298  0.57028018 -0.27906977 -0.27906977\n",
      "  -0.5        -1.          0.05544475 -0.64099048 -0.48654148 -0.18078767\n",
      "  -0.03227255 -0.37682847 -0.19998364 -1.        ]\n",
      " [-0.66995874 -0.13439342 -0.05704491  0.11330751 -0.00156168  0.05277817\n",
      "   0.03968823  0.03710235 -0.10268107  0.13864513  0.27649784 -0.03829457\n",
      "   0.14543812 -0.09958359 -0.09764393 -0.00921862 -0.56509565 -0.12456867\n",
      "  -0.05625962 -0.08713705 -0.15008255  0.32676169 -0.03450883 -0.11119347\n",
      "  -0.11811363 -0.1145068  -0.24359819 -0.62319372 -0.54732327 -0.00184701\n",
      "   0.15944434  0.47687893  0.2511703   0.46501116 -0.02325581 -0.02325581\n",
      "  -0.5        -1.          0.08990826 -0.62261633 -0.58954403 -0.06036382\n",
      "   0.08308169 -0.26440094 -0.2015964  -1.        ]\n",
      " [-0.70496312 -0.01060762  0.03700567  0.15807898  0.12104529  0.16262003\n",
      "   0.02549802 -0.02615439  0.52297353  0.15699878  0.19327796 -0.73782877\n",
      "   0.3910268  -0.14580234 -0.19935302 -0.03110535 -0.35076789  0.47080035\n",
      "   0.26117645 -0.0973517  -0.42798148  0.31460066 -0.03628357 -0.40543599\n",
      "   0.49295586  0.00975597 -0.53285524 -0.66174679 -0.54732327  0.09927088\n",
      "  -0.83629542  0.35771708  0.01969037  0.3818315  -0.40697674 -0.40697674\n",
      "  -0.5        -1.         -0.12350035 -0.52522822  0.11558229  0.02477354\n",
      "   0.30756681 -0.23743935 -0.31725632 -1.        ]\n",
      " [-0.34816852 -0.85865556 -0.79983504 -0.62756474 -0.67849621 -0.66777578\n",
      "   0.72341352  0.69055104 -0.35583771 -0.21107922 -0.02855097 -0.15360896\n",
      "   0.34231323  0.56292777  0.53458855  0.63769147 -0.19824737  0.03593705\n",
      "  -0.28540168 -0.27212235 -0.06022638 -0.03666945 -0.3242158   0.44970556\n",
      "  -0.41587804 -0.11363702 -0.10639107  0.1869617  -0.24367601 -0.07529848\n",
      "   0.15869393  0.1173972  -0.02140582  0.26111175  0.42818058  0.42818058\n",
      "  -0.5        -1.          0.26605504 -0.48791504 -0.728815   -0.10302409\n",
      "  -0.32167002 -0.27128053  0.08631114 -0.33333333]\n",
      " [-0.62770346 -0.46414043 -0.39452159 -0.22294583 -0.25366381 -0.29745273\n",
      "   0.26948891  0.23411662 -0.18209292  0.24602231  0.67607049 -0.06547781\n",
      "   0.52965974 -0.16813106 -0.19254449 -0.00709547 -0.25112274 -0.16120161\n",
      "  -0.2543462   0.08876021  0.43702853  0.80855834  0.05071186 -0.70431059\n",
      "  -0.20818763 -0.19131958 -0.18723244 -0.2569787  -0.59636668 -0.29537291\n",
      "   0.19172737 -0.16686781 -0.18021171  0.00741655  0.60875513  0.60875513\n",
      "   0.         -1.          0.48623853 -1.         -1.          0.25523714\n",
      "  -0.04210896  0.45464895 -0.60656188  0.27603576]\n",
      " [-0.72371546  0.63095687  0.54882131  0.82011255  0.71741312  0.77720028\n",
      "  -0.90926128 -0.89862109  0.25009668  0.06016899  0.22037962 -0.41010601\n",
      "   0.03643481 -0.49904437 -0.50444891 -0.45922203 -0.09689174  0.20429867\n",
      "   0.20869639  0.35687981 -0.21172756  0.74108378  0.50833833  0.02252581\n",
      "   0.32725753 -0.6415053  -0.19533395 -0.88897835 -0.71476795 -0.60217796\n",
      "   0.4862994   0.53938788  0.39575442  0.08756878  0.14728682  0.14728682\n",
      "  -0.5        -1.          0.26605504 -1.         -1.          0.32996627\n",
      "   0.16604739 -0.2965414  -0.66931924 -1.        ]\n",
      " [ 0.78247281 -0.66868097 -0.72518969 -0.40101394 -0.42338811 -0.42126823\n",
      "   0.59984252  0.54444921 -0.64328451 -0.20819534  0.35414108 -0.62083098\n",
      "   0.5295393   0.46741799  0.40692326  0.58468249  0.15148937 -0.43005638\n",
      "  -0.33552754 -0.18006619  0.04393566  0.77770533 -0.22457772  0.27497321\n",
      "  -0.68447937  0.5120877  -0.56283977  0.24674457  0.19951638 -0.4195852\n",
      "   0.19170452 -0.51157277 -0.61445395 -0.58101291  0.89767442  0.89767442\n",
      "  -0.5        -1.          0.12593828 -1.         -1.          0.84080449\n",
      "   0.67901677 -0.31145583 -0.86832791  0.33333333]\n",
      " [-0.12614077 -0.56571317 -0.53049111 -0.27456741 -0.38142029 -0.38446406\n",
      "   0.05666408  0.02809495  0.06897844 -0.253007    0.38956507  0.3118193\n",
      "   0.59542307 -0.40025248 -0.41594336 -0.19898212  0.19782757 -0.36306944\n",
      "  -0.21987877  0.60795795  0.36413357  0.56567398  0.58789358 -0.20276123\n",
      "   0.06919514 -0.49118221  0.58620489 -0.39009838 -0.59636668 -0.32075332\n",
      "   0.3498337  -0.13781448 -0.17816895 -0.03020396  0.69302326  0.69302326\n",
      "   0.5        -1.          0.63867325 -1.         -1.         -0.01275679\n",
      "  -0.51885965  0.24437233 -0.31050351 -0.3906309 ]\n",
      " [-0.18139767 -0.46301273 -0.39287197 -0.21068979 -0.26039782 -0.30363279\n",
      "   0.12034204  0.0882043   0.07956826  0.14542663  0.57754574  0.11785945\n",
      "   0.55254441 -0.44703325 -0.46326486 -0.24890467 -0.30960583 -0.21516873\n",
      "  -0.52497969  0.31291829  0.42895752  0.67795296  0.26567292 -0.64353162\n",
      "   0.07163987 -0.61023533  0.50143161 -0.4175847  -0.59636668 -0.22095163\n",
      "   0.34976667 -0.28020097 -0.12622434 -0.09112745  0.60875513  0.60875513\n",
      "   0.5        -1.          0.69123695 -1.         -1.         -0.02449437\n",
      "  -0.44887771  0.56392705 -0.39531671 -0.3906309 ]]\n",
      "[1.64890034 1.64028147 1.79367328 1.64658213 1.76719197 2.2830507\n",
      " 1.56300199 1.63824882 1.86125556 1.14312429 1.51470953]\n"
     ]
    }
   ],
   "source": [
    "Y_train,x_train_jd,x_train_md = randPepSelector(APs,judp,modp,smiles,x_train_jd,x_train_md,Y_train)                 # first we take a set of random peptides(10)\n",
    "print(x_train_jd)\n",
    "print(x_train_md)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(s,Y_pred):                      #takes the APs of top peptides and returns their location in judred parameters so we can look up their names by their location\n",
    "    y = []\n",
    "    for i in range(len(s)):\n",
    "        for j in range(len(Y_pred)):\n",
    "            if s[i] == Y_pred[j]:\n",
    "                if j not in y:\n",
    "                    y.append(j)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function, takes in training data(judred parameters and AP scores) and trains a SVM. The hyperparameters for this model are given in the supporting information of the paper. Then it predicts the AP score for all peptides using the judred data. We then return the names of the top 1207 peptides which are predicted. The number 1207 comes from the formula given in the paper.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block-1\n",
    "\n",
    "def topN(n,judp,x_train_jd,Y_train):                                                            #takes the judred parameters and the training data \n",
    "    svr = SVR(kernel='rbf',gamma='scale',C=100,epsilon=0.1,max_iter=-1,tol=0.0001,verbose=0)    #trains a svm(rbf)\n",
    "    svr.fit(x_train_jd,Y_train)\n",
    "    x = []\n",
    "    for i in range(8000):\n",
    "        x.append(np.array(judp.iloc[i]))\n",
    "    imputer = SimpleImputer(strategy='mean')                                                    #imputer removes NaN values from the data set\n",
    "    x_imputed = imputer.fit_transform(x)\n",
    "    Y_pred = svr.predict(x_imputed)                                                             #predicts AP scores for all peptides\n",
    "    s = sorted(Y_pred)\n",
    "    s = s[8000-n:]\n",
    "    y_loc = loc(s,Y_pred)\n",
    "    y_nam = ['']*len(y_loc)\n",
    "    for i in range(len(y_loc)):\n",
    "        y_nam[i] = judp.iloc[y_loc[i]].name\n",
    "    return y_nam   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the names of the predicted peptides, looks the data up using the name and adds it the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block-2\n",
    "\n",
    "def addToTrain(y_nam,APs,judp,modp,x_train_jd,x_train_md,Y_train):                                              #takes the predicted peptide in previous iteration and adds them to training data\n",
    "    for i in y_nam:\n",
    "        if np.array(APs.loc[i.decode(encoding='utf-8')]) not in Y_train:\n",
    "            x_train_jd = np.append(x_train_jd, [np.array(judp.loc[i])], axis=0)\n",
    "            x_train_md = np.append(x_train_md, np.array(modp.iloc[smiles.index[smiles['Peptides']==i.decode(encoding='utf-8')]]), axis=0)\n",
    "            Y_train = np.append(Y_train, np.array(APs.loc[i.decode(encoding='utf-8')]), axis=0)\n",
    "    return x_train_jd,x_train_md,Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains a svm on the mordred data of the peptides in training dataset and gives the names of the predicted peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block-3\n",
    "\n",
    "def topn(n,smiles, modp, y_nam_jd, x_train_md, Y_train):\n",
    "    svr = SVR(kernel='rbf',gamma='scale',C=100,epsilon=0.1,max_iter=-1,tol=0.0001,verbose=0)    #trains a svm(rbf)\n",
    "    svr.fit(x_train_md,Y_train)\n",
    "    x = np.empty((0,modp.shape[1]))\n",
    "    for i in y_nam_jd:\n",
    "        x = np.append(x, np.array(modp.iloc[smiles.index[smiles['Peptides']==i.decode(encoding='utf-8')]]), axis=0)\n",
    "    Y_pred = svr.predict(x)                                                                     #predicts AP scores for all peptides\n",
    "    pred = pd.DataFrame(data=Y_pred,index=y_nam_jd,columns=['prediction'])\n",
    "    y_nam = np.array(pred['prediction'].nlargest(n=n).index)\n",
    "    return y_nam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function identifies if a peptide present in 'sap' the list of self assembling peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pepidentifier(y_nam,sap,a):\n",
    "    for j in y_nam:\n",
    "        b = j.decode(encoding='utf-8')\n",
    "        for i in sap:            \n",
    "            if b == i:\n",
    "                a.append(b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the active learning algorithm, using the functions defined above. In step 1, block 1 is called. In step 2, block 3 is called. In step 3, block 2 is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'HSE-ILE-MET' b'HSE-MET-ILE' b'ILE-HSE-MET' b'ILE-MET-HSE'\n",
      " b'MET-HSE-ILE' b'MET-ILE-HSE' b'HSE-VAL-MET' b'HSE-MET-VAL'\n",
      " b'MET-HSE-VAL' b'ILE-PHE-MET']\n",
      "[b'TYR-MET-LEU' b'TYR-CYS-LEU' b'TYR-LEU-MET' b'TYR-LEU-CYS'\n",
      " b'VAL-GLY-MET' b'MET-GLY-LEU' b'HSE-GLY-MET' b'TYR-MET-CYS'\n",
      " b'TYR-CYS-MET' b'TYR-MET-MET']\n",
      "[b'VAL-MET-HSE' b'TYR-MET-MET' b'HSE-MET-LEU' b'VAL-HSE-MET'\n",
      " b'TYR-MET-ILE' b'ASN-MET-PHE' b'TYR-ALA-MET' b'VAL-CYS-HSE'\n",
      " b'HSE-CYS-LEU' b'MET-HSE-LEU']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'HSE-MET-LEU'\n",
      " b'TYR-ILE-MET' b'HSE-CYS-LEU' b'VAL-MET-HSE' b'TYR-MET-VAL'\n",
      " b'TYR-MET-CYS' b'TYR-CYS-MET']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS']\n",
      "['ILE-PHE-PHE', 'TRP-LEU-LEU', 'PHE-PHE-MET', 'MET-PHE-PHE', 'PRO-CYS-PHE', 'LEU-CYS-PHE', 'ILE-MET-TRP', 'SER-CYS-TRP', 'VAL-PHE-PHE', 'PHE-TYR-ILE', 'VAL-ALA-TRP', 'PRO-PHE-PHE', 'SER-PHE-TRP', 'GLY-PHE-PHE']\n",
      "[b'TYR-MET-MET' b'TYR-MET-ILE' b'TYR-ALA-MET' b'TYR-MET-VAL'\n",
      " b'TYR-ILE-MET' b'HSE-MET-LEU' b'HSE-CYS-LEU' b'TYR-MET-ALA'\n",
      " b'TYR-CYS-MET' b'TYR-MET-CYS' b'PHE-GLY-ILE' b'TYR-CYS-ILE'\n",
      " b'VAL-MET-HSE' b'TYR-VAL-MET' b'TYR-MET-LEU' b'MET-ALA-PHE'\n",
      " b'VAL-GLY-PHE' b'ASN-CYS-ILE' b'VAL-HSE-MET' b'VAL-MET-PHE'\n",
      " b'HSE-LEU-CYS' b'ASN-MET-ILE' b'TYR-LEU-MET' b'HSE-LEU-MET'\n",
      " b'HSE-MET-VAL' b'PHE-GLY-VAL' b'HSE-MET-ILE' b'ASN-PHE-VAL'\n",
      " b'TYR-ILE-CYS' b'TYR-CYS-VAL' b'TYR-PRO-MET' b'VAL-CYS-HSE'\n",
      " b'MET-MET-PHE' b'TYR-MET-PHE' b'MET-HSE-LEU' b'ASN-VAL-PHE'\n",
      " b'ILE-GLY-PHE' b'VAL-PHE-MET' b'ASN-CYS-LEU' b'VAL-CYS-PHE']\n"
     ]
    }
   ],
   "source": [
    "def runAL(num_iter,APs,judp,smiles,modp,x_train_jd,x_train_md,Y_train,sap):\n",
    "    a = []                                                                                            #array to keep track if any self-assembling peptide is found\n",
    "    for i in range(num_iter):                                                                         #a for loop is for the number of iterations as given in input\n",
    "        y_nam_jd = topN(math.ceil((np.log(3)**2) * 1000),judp,x_train_jd,Y_train)                     #Step 1: we get the names of top 1207 peptides which are predicted by judred model by calling the function 'topN'\n",
    "        y_nam= topn(10,smiles, modp, y_nam_jd, x_train_md, Y_train)                                   #Step 2: from the peptides predicted by judred model we predict the top 10 AP scoring peptides as predicted by the mordred model\n",
    "        print(y_nam)\n",
    "        a = pepidentifier(y_nam_jd,sap,a)                                                                #we call the peptide identifier function which checks if any self-assembling peptide is predicted.\n",
    "        x_train_jd,x_train_md,Y_train = addToTrain(y_nam,APs,judp,modp,x_train_jd,x_train_md,Y_train) #Step 3: we add the top 10 peptides to the training set.\n",
    "    b = []\n",
    "    for j in range(len(a)):\n",
    "        if a[j] not in b:\n",
    "            b.append(a[j])\n",
    "    print(b)\n",
    "    return topn(40,smiles, modp, y_nam_jd, x_train_md, Y_train)                                       #after the last iteration we print the top 40 peptides predicted by the mordred model.\n",
    "\n",
    "y_nam = runAL(20,APs,judp,smiles,modp,x_train_jd,x_train_md,Y_train,sap)                              #we run the active learning process for 80 iteration.\n",
    "print(y_nam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we find that at max only one peptide is found in the process for 80 iterations, whereas 20 peptides where found within 15 iterations in the paper.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
